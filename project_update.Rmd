---
title: "Baseball performance modeled with skeletal and force plate data"
subtitle: "Driveline Baseball"
author: "Eli Westrick"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    fig_caption: true
    self_contained: true
    css: styles.css   # optional custom styling
always_allow_html: yes
header-includes:
- \usepackage[section]{placeins}
- \usepackage{fixltx2e}
- \usepackage{longtable}
- \usepackage{pdflscape}
- \usepackage{graphicx}
- \usepackage{caption}
- \usepackage{gensymb}
- \usepackage{subcaption}
- \DeclareUnicodeCharacter{2264}{$\pm$}
- \DeclareUnicodeCharacter{2265}{$\geq$}
- \usepackage{fancyhdr}
- \usepackage{lipsum}
#- \pagestyle{fancy}
#- \fancyhead{DRAFT}
---

```{r setup, include = FALSE}

#Please modify the chunk set up as you wish, some options are in # below

options("digits" = 5)
options("digits.secs" = 3)


knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 6, # the width for plots created by code chunk
  fig.height = 4.5, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 300,
  dev = 'png',
  # results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = TRUE) # if FALSE knitr won't display warning messages in the doc

#Add packages
library(tidyr)
library(dplyr)
library(ggplot2)

#Set universal them for figures, you can use your own
theme_set(theme_light())

```

# Executive Summary

* Overall goal is to create a machine learning model with predictive power for launch angle, distance, and exit velocity of a batted ball.
* This will be accomplished using skeletal data from the open biomechanics project at driveline baseball.
* Data cleaning and EDA was performed to create a dataset compatible with machine learning models.
* EDA gave distributions for response variables. The main response variable, exit velocity, has a mean at about 91, with a SD of 7.19.
* Data was filtered to only include right handed hitters.
* Data was shaped to include 100 time series observations from each swing, with the final point being the point of contact with the baseball.
* GNN - GRU hybrid model was created to try and predict all 3 response variables from the skeletal time series data, but had incredibly high loss values.
* To minimize loss, the output variables were split, and it was discovered that while launch angle and distance of the ball were not able to be predicted
with much accuracy, exit velocity was.
* GNN - GRU hybrid model was created to just predict exit velocity based on the skeletal time series data. 
* Hyperparameters were tuned with pyswarm, giving the following hyperparameters. Hidden Dimensions = 114, Learning rate = 0.0819337, Number of layers = 2.
* Model was trained and tested, getting the following results: MSE: 48.9, MAE: 5.49.
* GNN - GRU model was re-evaluated with a different edge matrix that used spatial dependencies between the nodes. This got the following results: MSE: 49.1973, MAE: 5.4552.
* Graph transformer model was created with full attention to edges, meaning all nodes were connected to each other with edges. This model got the following results: MSE: 49.45, MAE: 5.42.
* Overall, the graph transformer was the strongest model, although both model types were able to predict exit velocity well within a single standard deviation. 


 


# Abstract

In this project, a deep analysis of skeletal data from Driveline Baseball was performed to try and create a model capable of predicting exit velocity, launch angle, and batted ball distance. This was based on landmark nodes that were placed on hitters, and kept track of the coordinates of various body parts during a baseball swing. Initial data cleaning and EDA was performed to create a dataset that would work for machine learning. This dataset consisted of 411 total swings, filtered to only include right handed hitters. To minimize noise in the modeling, each swing was split into 100 time series points, instead of the usual 500 - 800 points that were in the original dataset. This was to make sure that each swing had the same number of points, but also that the last point of each swing was the point of contact with the ball. Two different machine learning models were created, a GNN - GRU hybrid model, and a graph transformer model. Pyswarm was used to tune hyperparameters of the GNN model. After some testing and changes, the best models that could be created were a GNN - GRU model with edges based on spatial dependencies that would just predict exit velocity, and a graph transformer model, also predicting only exit velocity. The GNN - GRU model had an MSE of 49.1973 and an MAE of 5.4552. The graph transformer model had an MSE of 49.45 and an MAE of 5.42. In the dataset, the standard deviation of exit velocity is 7.19, meaning both models have very strong predictive capabilities. 
   
# Introduction

Baseball data is a growing field of study, with leagues such as the MLB and minor leagues as well as performance facilities staking a lot on the findings. Of this data, skeletal data is an emerging type that is used a lot in maximizing performance. Nodes placed on someones body can track certain joint positions, angles, and velocities over time as they perform baseball related movements. Using this data, as well as results from other recorded variables, you are able to see what in your mechanics might be helping or hurting your performance. In this study, we are looking at the skeletal data of hitting mechanics, while a person goes through the motion of hitting a baseball. We are looking to maximise exit velocity and distance, as well as predict the launch angle of the ball using this data. These are very relevant areas to look at, as they are the most common performance metrics in the MLB, and are often used to determine which players should be on the field and which should be on the bench. In performance facilities, having a method of creating a good swing that is data driven will improve business, and the amount of memberships you are able to obtain. 
   
# Data Science Methods

In this project, we use basic data cleaning and EDA to create a dataset that works for modeling. We also create some basic visualizations to give readers a better feel for the data. We then go into some advanced modeling using machine learning, starting with a GNN-GRU hybrid model, before moving into a graph transformer model. 

# Exploratory Data Analysis

## Explanation of your data set

Data was downloaded from Driveline open github repo: https://github.com/drivelineresearch/openbiomechanics/blob/main/baseball_hitting/README.md

The data we use is split into 3 data sets; metadata, hittrax, and landmarks. All of these sets span 677 baseball swings. In the hittrax set, there are 677 swings with outcome variables, such as exit velocity, launch angle, and distance. In the metadata set, there is basic information on the hitter, such as their height, weight, type of bat, and handedness. In the landmarks set, we get a time series of about 800 measurements per swing. The time series tracks the landmark coordinates in 3 axis of the 14 nodes used in the study. This set will serve as the basis for the GNN-GRU model as well as the graph transformer.  

## Background Knowledge and terminology

Data and bio metric analysis is becoming a very large part of both MLB and performance centers. Bio metric analysis is usually done with a sort of skeletal suit that athletes will wear during swinging the bat or throwing the ball. This suit will be able to measure where certain points on their body are at all times during the activity. In this study, we will look at swinging the bat. The response variables we will be using are the following. Exit Velocity: This refers to the speed in mph that the ball is hit off the bat. Launch Angle: This refers to the angle that the ball comes off the bat. 0 degrees of launch angle would represent a ball hit on a line straight forward, while 90 degrees would be straight up in the air. Distance: This refers to the distance in feet that the ball is measures to have gone off that bat. This is done using calculations from a hittrax machine, as are the readings of the other variables. Blast Bat Speed: This refers to the speed in mph of the "Sweet spot" of the bat at the time that the bat hits the ball. The sweet spot is the part of the bat where contact is desired, a thicker part of the bat spanning about 4-5 inches, located about 2 inches off the end of the bat. These variables are all important parts of analysis, as they directly affect the likelihood of reaching base, or hitting a home run on a batted ball.

## Data Cleaning


Importing data
```{r}
library(readr)
setwd("C:/Users/eliwe/Downloads/452_data")
metadata <- read_csv("metadata.csv")
hittrax <- read_csv("hittrax.csv")
landmarks <- read_csv("landmarks.csv")

```

```{r}

metadata%>%head(5)
hittrax%>%head(5)
landmarks%>%head(5)

metadata <- metadata %>%
  filter(hitter_side == "R") 
sd(metadata$exit_velo_mph_x, na.rm = TRUE)

nrow(landmarks)
ncol(landmarks)
```

Force_plate: 1,375,074 observations of 11 variable, time series format.


Landmarks: 469,377 observations of 65 variables, time series format.


Codebook for landmarks:

lhjc:	left hand joint center <br>
left_hip:	left hip joint center <br>
lsjc:	left shoulder joint center <br>
lejc:	left elbow joint center <br>
lkjc:	left knee joint center <br>
lajc:	left ankle joint center <br>
lwjc:	left wrist joint center <br>
rhjc:	right hand joint center <br>
right_hip:	right hip joint center <br>
rsjc:	right shoulder joint center <br>
rejc:	right shoulder joint center <br>
rkjc:	right knee joint center <br>
rajc:	right ankle joint center <br>
rwjc:	right wrist joint center <br>


hittrax: 604 observations of 13 variables.
metadata: 677 observations of 13 variables.



* What you had to do to clean your data

```{r}
sum(is.na(hittrax$la))
sum(is.na(hittrax$dist))
sum(is.na(metadata$exit_velo_mph_x))


#it looks like there are a few observations where
#the hitter didn't hit the ball, which we want to omit
metadata <-na.omit(metadata)

filter_data <- hittrax %>%
  select("la", "dist", "session_swing", "strike_zone")

filter_data_2 <- metadata %>%
  select("session_swing", "hitter_side", "blast_bat_speed_mph_x", "exit_velo_mph_x")


df <- merge(filter_data, filter_data_2, by = "session_swing")

final_df <- merge(landmarks, df, by = "session_swing")

```

```{r}
#for use in python with a GNN, we need data where there are the same number
#of instances for each session swing. Right now some swings are longer than 
#others, which results in more time for each swing

table(metadata$hitter_side)
#we also need all of the same handed hitters, because the coordinates will 
#be way different based on the side you are hitting from. We will have to 
#filter down to just right handers in this dataset.

data <- final_df %>%
  filter(hitter_side == "R") %>%
  separate(session_swing, into = c("player", "swing"), sep = "_", convert = TRUE, remove = FALSE)

mean(data$contact_time)

#it really only makes sense if all of the swings stop right as they are making contact
#or the model will likely take into account the follow through of the swing, 
#which theoretically should not have any impact on our response variables.
#even if it were to have an impact, it is not necessarily something that we want 
#to look at in this project

data <- data %>%
  group_by(session_swing) %>%
  filter(time < contact_time)

library(purrr)

# Assuming df is already filtered to only time <= contact_time
resampled_df <- data %>%
  group_by(session_swing) %>%
  arrange(time) %>%
  group_split() %>%
  map_dfr(function(group) {
    # Create new time points (ending at contact)
    t_new <- seq(min(group$time), max(group$time), length.out = 100)
    
    # Select numeric columns to interpolate
    numeric_vars <- group %>%
      select(where(is.numeric), -time)

    # Keep only columns with ≥2 non-NA values
    valid_vars <- numeric_vars %>%
      select(where(~ sum(!is.na(.)) >= 2))
    
    # Interpolate each valid variable
    interp_vars <- valid_vars %>%
      map_dfc(~ approx(group$time, .x, xout = t_new, rule = 2)$y)
    
    # Recombine swing_id, time, and interpolated data
    tibble(
      session_swing = unique(group$session_swing),
      time = t_new
    ) %>%
      bind_cols(interp_vars)
  })

resampled_df <- resampled_df %>%
  select(-strike_zone, - blast_bat_speed_mph_x, -fp_10_time, -fp_100_time, -blast_hand_y, -blast_hand_x, -blast_hand_z, -sweet_spot_x, -sweet_spot_y, - sweet_spot_z, -thorax_ap_x, -thorax_ap_y, -thorax_ap_z, -thorax_dist_x, -thorax_dist_y, -thorax_dist_z, -thorax_prox_x, -thorax_prox_y, -thorax_prox_z)

#now we have a good dataset that we can pass into our machine learning script





```


## Data Vizualizations

* Vizualizations of your data

```{r}



plot1 <- ggplot(metadata%>%filter(exit_velo_mph_x > 70), aes(x = exit_velo_mph_x)) +
  geom_histogram()
plot1


plot2 <- ggplot(hittrax, aes(x = dist)) +
  geom_histogram()
plot2

plot3 <- ggplot(hittrax, aes(x = la)) +
  geom_histogram()
plot3


 


```
  
# Statistical Learning: Modeling \& Prediction

Code and output in the python .md file. 
   
# Discussion

Through use of some machine learning techniques, we were able to create a model that predicted exit velocity with an MAE of about 5.4. Based on the type of data, and how it included a single dimension outcome as well as a time series input, GNN modeling and graph transformers were set up to perform very well. It was also ideal that the nodes (body parts) in the data contained natural edges. That is, there were well defined spatial dependencies between nodes that would make sense as edges, and would help with modeling. Graph transformer models are also structured with attention that will allow them to see complicated patterns in the data, such as how the lower half nodes early in the swing can affect upper half nodes later in the swing. 
  
# Conclusions

Overall, we were able to prove that GNN models and graph transformers have a lot of utility with skeletal data. This is a great proof of concept to warrant further investigation, and deeper models. The next steps for this project would be to dive deeper into the graph transformer model, to try and maximize the predictive power. A few ways to do this would be to expand the input data to include more time series points, to play around more with the architecture and layers of the model to better fit the data, or to try and uniquely configure the edge matrix to match up with the GNN model. 
   
# Acknowledgments

Special thanks to Chen Yang, Yan Chen, and Pengfei Jin for sharing their code and their models. Their paper is amazing and is linked in the references.
   
# References

Yang, Chen. Chen, Yan, Jin, Pengfei. “Leveraging Graph Neural Networks and Gate Recurrent Units for Accurate and Transparent Prediction of Baseball Pitching Speed.” Nature News, Scientific Reports, 5 Mar. 2025, www.nature.com/articles/s41598-025-88284-x. 

Driveline, Baseball Research. “The Open Biomechanics Project Github Repo.” GitHub, Driveline Baseball, github.com/drivelineresearch/openbiomechanics/tree/main/high_performance. Accessed 28 July 2025. 

“Pyswarm Documentation.” Welcome to PySwarms’s Documentation! - PySwarms 1.3.0 Documentation, pyswarms.readthedocs.io/en/latest/. Accessed 28 July 2025. 

“PYTORCH Documentation¶.” PyTorch Documentation - PyTorch 2.7 Documentation, docs.pytorch.org/docs/stable/index.html. Accessed 28 July 2025. 